{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<-<-<----- C H A M E L E O N   M E T A D A T A   P Y T H O N   S C R I P T  ---->->->\n",
      ">>>>>                                                                           <<<<<\n",
      ">>>>>               ROUTINE NAME:                   DSB-3-LAB-1-A               <<<<<\n",
      ">>>>>                                                                           <<<<<\n",
      ">>>>>               Start Time:        2018-09-06 12:42:33.930747               <<<<<\n",
      ">>>>>                                                                           <<<<<\n",
      "<-<-<-<-<-<-<-<-<-------  E X E C U T I O N   B E G I N S  ------->->->->->->->->->-> \n",
      "\n",
      "Request Sent to SEC Edgar: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001631574&type=10-K \n",
      "\n",
      "Reply Received from SEC Edgar (Documents Button Target URL): \n",
      "['https://www.sec.gov/Archives/edgar/data/1631574/000119312518157593/0001193125-18-157593-index.htm',\n",
      " 'https://www.sec.gov/Archives/edgar/data/1631574/000119312518139325/0001193125-18-139325-index.htm',\n",
      " 'https://www.sec.gov/Archives/edgar/data/1631574/000156459018005308/0001564590-18-005308-index.htm',\n",
      " 'https://www.sec.gov/Archives/edgar/data/1631574/000119312517148202/0001193125-17-148202-index.htm',\n",
      " 'https://www.sec.gov/Archives/edgar/data/1631574/000156459017004649/0001564590-17-004649-index.htm',\n",
      " 'https://www.sec.gov/Archives/edgar/data/1631574/000119312516569179/0001193125-16-569179-index.htm',\n",
      " 'https://www.sec.gov/Archives/edgar/data/1631574/000119312516523890/0001193125-16-523890-index.htm']\n",
      "_____________________________________________________________________ \n",
      "\n",
      "Directory Already Exists:  c:/dmz-ftp/sec-edgar/0001631574\n",
      "_____________________________________________________________________ \n",
      "\n",
      "Storage Hierarchy (windows-dsb-3-lab-1-b) with Nodes Set As:\n",
      " ~ local_server_os   =  Windows\n",
      " ~ ~ local_server_ip   =  192.168.1.114\n",
      " ~ ~ ~ local_root_drive   =  c\n",
      " ~ ~ ~ ~ dir_purpose        =  dmz-ftp\n",
      " ~ ~ ~ ~ ~ dir_source         =  sec-edgar\n",
      " ~ ~ ~ ~ ~ ~ dir_src_sg1        =  cik\n",
      " ~ ~ ~ ~ ~ ~ ~ dir_src_sg2        =  secNum\n",
      " ~ ~ ~ ~ ~ ~ ~ ~ JOB ID             =  18-09-06-12-42\n",
      "_____________________________________________________________________ \n",
      "\n",
      "sec_edgar_request_metadata:\n",
      "['DSB-3-LAB-1-A',\n",
      " '2018-09-06 12:42:33.930747',\n",
      " '18-09-06-12-42',\n",
      " '0001631574',\n",
      " '10-K',\n",
      " '&dateb NONE',\n",
      " '&count NONE',\n",
      " 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001631574&type=10-K',\n",
      " 'windows-dsb-3-lab-1-b',\n",
      " 'c:/dmz-ftp/sec-edgar/0001631574',\n",
      " 'c:/dmz-ftp/sec-edgar/0001631574/PROTOCOL-DSB-3-LAB-1_18-09-06-12-42_Edgar.request',\n",
      " 'c:/dmz-ftp/sec-edgar/0001631574/PROTOCOL-DSB-3-LAB-1_18-09-06-12-42_Edgar.reply']\n",
      " \n",
      "\n",
      "<-<-<-<-<-<-<-<-<-<----  E X E C U T I O N   C O M P L E T E  ---->->->->->->->->->->\n",
      ">>>>>                                                                           <<<<<\n",
      ">>>>>               R O U T I N E ................. DSB-3-LAB-1-A               <<<<<\n",
      ">>>>>                                                                           <<<<<\n",
      ">>>>>         End Time:  2018-09-06 12:42:35.083223                             <<<<<\n",
      ">>>>>       Start Time:  2018-09-06 12:42:33.930747                             <<<<<\n",
      ">>>>>                    ~~~~~~~~~~~~~~~~~~~~~~~~~~                             <<<<<\n",
      ">>>>>   Execution Time:              0:00:01.152476                             <<<<<\n",
      ">>>>>                                                                           <<<<<\n",
      "<-<-<-<-<-<-<-<-<-<----  E X E C U T I O N   C O M P L E T E  ---->->->->->->->->->->\n",
      " \n",
      "_____________________________________________________________________ \n",
      "\n",
      "<-<-<----- C H A M E L E O N   M E T A D A T A   P Y T H O N   S C R I P T  ---->->->\n",
      ">>>>>                                                                           <<<<<\n",
      ">>>>>               ROUTINE NAME:                   DSB-3-LAB-1-B               <<<<<\n",
      ">>>>>                                                                           <<<<<\n",
      ">>>>>               Start Time:        2018-09-06 12:42:35.084231               <<<<<\n",
      ">>>>>                                                                           <<<<<\n",
      "<-<-<-<-<-<-<-<-<-------  E X E C U T I O N   B E G I N S  ------->->->->->->->->->-> \n",
      "\n",
      "job_log_suffix IN-MEMORY from past step: 18-09-06-12-42\n",
      " ~ Setting JOB ID (job_log_suffix) to:  18-09-06-12-42  \n",
      "\n",
      "PATH Not Found:      c:/dmz-ftp/sec-edgar/0001631574/0001193125-18-157593/18-09-06-12-42\n",
      " ~ Directory Created:   c:/dmz-ftp/sec-edgar/0001631574/0001193125-18-157593/18-09-06-12-42  \n",
      "\n",
      "PATH Not Found:      c:/dmz-ftp/sec-edgar/0001631574/0001193125-18-139325/18-09-06-12-42\n",
      " ~ Directory Created:   c:/dmz-ftp/sec-edgar/0001631574/0001193125-18-139325/18-09-06-12-42  \n",
      "\n",
      "PATH Not Found:      c:/dmz-ftp/sec-edgar/0001631574/0001564590-18-005308/18-09-06-12-42\n",
      " ~ Directory Created:   c:/dmz-ftp/sec-edgar/0001631574/0001564590-18-005308/18-09-06-12-42  \n",
      "\n",
      "PATH Not Found:      c:/dmz-ftp/sec-edgar/0001631574/0001193125-17-148202/18-09-06-12-42\n",
      " ~ Directory Created:   c:/dmz-ftp/sec-edgar/0001631574/0001193125-17-148202/18-09-06-12-42  \n",
      "\n",
      "PATH Not Found:      c:/dmz-ftp/sec-edgar/0001631574/0001564590-17-004649/18-09-06-12-42\n",
      " ~ Directory Created:   c:/dmz-ftp/sec-edgar/0001631574/0001564590-17-004649/18-09-06-12-42  \n",
      "\n",
      "PATH Not Found:      c:/dmz-ftp/sec-edgar/0001631574/0001193125-16-569179/18-09-06-12-42\n",
      " ~ Directory Created:   c:/dmz-ftp/sec-edgar/0001631574/0001193125-16-569179/18-09-06-12-42  \n",
      "\n",
      "PATH Not Found:      c:/dmz-ftp/sec-edgar/0001631574/0001193125-16-523890/18-09-06-12-42\n",
      " ~ Directory Created:   c:/dmz-ftp/sec-edgar/0001631574/0001193125-16-523890/18-09-06-12-42  \n",
      "\n",
      "_____________________________________________________________________ \n",
      "\n",
      "Storage Hierarchy (windows-dsb-3-lab-1-b) with Nodes Set As:\n",
      " ~ local_server_os   =  Windows\n",
      " ~ ~ local_server_ip   =  192.168.1.114\n",
      " ~ ~ ~ local_root_drive   =  c\n",
      " ~ ~ ~ ~ dir_purpose        =  dmz-ftp\n",
      " ~ ~ ~ ~ ~ dir_source         =  sec-edgar\n",
      " ~ ~ ~ ~ ~ ~ dir_src_sg1        =  cik\n",
      " ~ ~ ~ ~ ~ ~ ~ dir_src_sg2        =  secNum\n",
      " ~ ~ ~ ~ ~ ~ ~ ~ JOB ID             =  18-09-06-12-42\n",
      "_____________________________________________________________________ \n",
      "\n",
      " \n",
      "\n",
      "<-<-<-<-<-<-<-<-<-<----  E X E C U T I O N   C O M P L E T E  ---->->->->->->->->->->\n",
      ">>>>>                                                                           <<<<<\n",
      ">>>>>               R O U T I N E ................. DSB-3-LAB-1-B               <<<<<\n",
      ">>>>>                                                                           <<<<<\n",
      ">>>>>         End Time:  2018-09-06 12:43:37.680022                             <<<<<\n",
      ">>>>>       Start Time:  2018-09-06 12:42:35.084231                             <<<<<\n",
      ">>>>>                    ~~~~~~~~~~~~~~~~~~~~~~~~~~                             <<<<<\n",
      ">>>>>   Execution Time:              0:01:02.595791                             <<<<<\n",
      ">>>>>                                                                           <<<<<\n",
      "<-<-<-<-<-<-<-<-<-<----  E X E C U T I O N   C O M P L E T E  ---->->->->->->->->->->\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import datetime\n",
    "#~~~ SET VARIABLES (BELOW) BEFORE EXECUTION (all variables are manditory except 'WSS SEC Edgar Optional' variables)\n",
    "#~~~ #~~~ These variables also exist in-line within two scripts below but have been grouped here to make the lab less complex\n",
    "#~~~ #~~~ To find them in their original position in the code, search for: #--- >>>>> COMMENTED OUT FOR LAB <<<<<\n",
    "#~~~\n",
    "\n",
    "storage_hierarchy_name            = 'windows-dsb-3-lab-1-b'\n",
    "\n",
    "job_log_prefix                    = 'PROTOCOL-DSB-3-LAB-1'\n",
    "job_log_suffix                    = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "\n",
    "local_server_os                   = 'Windows'        #~~~~ Operating System (not used when building directory names)\n",
    "local_server_ip                   = '192.168.1.114'  #~~~~ IP of Server which will store the Edgar datasets\n",
    "local_root_drive                  = 'c'              #~~~~ Root drive on server which will store the Edgar datasets\n",
    "dir_purpose                       = 'dmz-ftp'        #~~~~ 1st directory level (i.e. c:/dmz-ftp)\n",
    "dir_source                        = 'sec-edgar'      #~~~~ 2nd directory level (i.e. c:/dmz-ftp/sec-edgar)\n",
    "dir_src_sg1                       = 'cik'            #~~~~ 3rd directory level (i.e. c:/dmz-ftp/sec-edgar/cik)\n",
    "dir_src_sg2                       = 'secNum'         #~~~~ 4th directory level (i.e. c:/dmz-ftp/sec-edgar/cik/secNum)\n",
    "\n",
    "#~~~ #~~~ WSS SEC Edgar Manditory Variables\n",
    "\n",
    "#~~~ #~~~ To look up the CIK for another company, visit this URL---> https://www.sec.gov/edgar/searchedgar/companysearch.html\n",
    "sec_company_cik                = '0001631574'      #-- &CIK (Make sure to include leading zeros)\n",
    "\n",
    "sec_edgar_request_url_prefix      = 'https://www.sec.gov'\n",
    "sec_edgar_document_request_prefix = 'Archives/edgar/data'\n",
    "\n",
    "#~~~ #~~~ WSS SEC Edgar Optional Variables\n",
    "\n",
    "filing_type                    = '10-K'                  #-- &type URL variable (Filter on type of fileing)\n",
    "until_date                     = ' '    #-- Format: 'YYYYMMDD'   #-- &dateb URL variable (Filter on maximum filing date requested)\n",
    "filing_count                   = ' '    #-- Format: '99'         #-- &count URL variable (Limit the number of filings to return)\n",
    "\n",
    "#~~~\n",
    "#~~~ SET VARIABLES (ABOVE) BEFORE EXECUTION (all variables are manditory except 'WSS SEC Edgar Optional' variables)\n",
    "#~~~ #~~~ These variables also exist in-line within two scripts below but have been grouped here to make the lab less complex\n",
    "#~~~ #~~~ To find them in their original position in the code, search for: #--- >>>>> COMMENTED OUT FOR LAB <<<<<\n",
    "\n",
    "\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print ('<-<-<----- C H A M E L E O N   M E T A D A T A   P Y T H O N   S C R I P T  ---->->->')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('>>>>>               ROUTINE NAME:                   DSB-3-LAB-1-A               <<<<<')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('>>>>>               Start Time:       ', start_time, '              <<<<<')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('<-<-<-<-<-<-<-<-<-------  E X E C U T I O N   B E G I N S  ------->->->->->->->->->-> \\n')\n",
    "\n",
    "#### \n",
    "####----- Python Environment Division\n",
    "#### \n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#### \n",
    "####----- Working Storage Section (WSS)\n",
    "#### \n",
    "\n",
    "####----- DISCLAIMER  - Selecting CIK '0001631574' was completely random.\n",
    "#-------- It was only selected only company because they were at the top of Edgar's most recent filings list the day I wrote this student lab\n",
    "#-------- Change the 'sec_company_cik' parameter below to retrieve a different company's SEC filings from Edgar.\n",
    "#-------- \n",
    "\n",
    "#~~~ #~~~ Storage Hierarchy Variables\n",
    "\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<storage_hierarchy_name            = 'windows-dsb-3-lab-1-b'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<job_log_prefix                    = 'DSB-3-LAB-1-A'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<job_log_suffix                    = job_log_suffix\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<local_server_os                   = 'Windows'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<local_server_ip                   = '192.168.1.114'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<local_root_drive                  = 'c'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<dir_purpose                       = 'dmz-ftp'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<dir_source                        = 'sec-edgar'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<dir_src_sg1                        = 'cik'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<dir_src_sg2                        = 'secNum'\n",
    "\n",
    "####\n",
    "##----------------- WSS Static Values Variables\n",
    "#### \n",
    "\n",
    "##------------------------ WSS SEC Edgar Manditory\n",
    "#~~~ \n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<sec_company_cik                = '0001631574'      #-- &CIK URL variable - Make sure to include leading zeros\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<sec_edgar_request_url_prefix      = 'https://www.sec.gov'\n",
    "\n",
    "##------------------------ WSS SEC Edgar Optional\n",
    "\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<filing_type                    = '10-K'                  #-- &type URL variable (Filter on type of fileing)\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<until_date                     = ' '    #-- Format: 'YYYYMMDD'   #-- &dateb URL variable (Filter on maximum filing date requested)\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<filing_count                   = ' '    #-- Format: '99'         #-- &count URL variable (Limit the number of filings to return)\n",
    "\n",
    "\n",
    "#~~~ Initialize other variables used to build directory and file names based on request sent to Edgar\n",
    "\n",
    "local_DMZ_sec_cik_secNum_dir      = ' '\n",
    "\n",
    "doc_buttons_write_dir             = ' '\n",
    "\n",
    "doc_buttons_write_request_file    = ' '\n",
    "doc_buttons_write_reply_file      = ' '\n",
    "\n",
    "#### \n",
    "####----- Python Procedure Division\n",
    "#### \n",
    "\n",
    "#~~~ Build the request (as a URL) which will be setn to the SEC Edgar System adding any optional parameters if provided above\n",
    "\n",
    "if not filing_type                   == ' ':\n",
    "    sec_edgar_request_url            = ('https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' \n",
    "                                        + sec_company_cik \n",
    "                                        + '&type=' \n",
    "                                        + filing_type)\n",
    "    \n",
    "if not until_date                    == ' ':\n",
    "    sec_edgar_request_url            = (sec_edgar_request_url \n",
    "                                        + '&dateb=' \n",
    "                                        + until_date)\n",
    "\n",
    "if not filing_count                  == ' ':\n",
    "    sec_edgar_request_url            = (sec_edgar_request_url \n",
    "                                        + '&count' \n",
    "                                        + filing_count)\n",
    "\n",
    "\n",
    "#~~~ Build the directory prefix (for ease of reuse) and the directory into which the output files will be written\n",
    "output_os_dir_purpose_src_prefix = (local_root_drive + ':' + '/' \n",
    "                                    + dir_purpose + '/'\n",
    "                                    + dir_source)    \n",
    "    \n",
    "doc_buttons_write_dir            = (output_os_dir_purpose_src_prefix + '/' \n",
    "                                    + sec_company_cik + '/')\n",
    "####\n",
    "##----------------- Dictionaries, Data Frames, Files Etc.\n",
    "#### \n",
    "\n",
    "sec_edgar_request_metadata     = []\n",
    "sec_edgar_doc_buttons_list     = []\n",
    "\n",
    "\n",
    "#~~~ SEND REQUEST to and RECEIVE PAYLOAD from SEC Edgar\n",
    "with urllib.request.urlopen(sec_edgar_request_url) as edgar_request_url:\n",
    "    edgar_request_url_read = edgar_request_url.read() \n",
    "    \n",
    "#~~~ Parse the PAYLOAD from SEC Edgar using BeautifulSoup's HTML parser    \n",
    "    edgar_request_soup = BeautifulSoup(edgar_request_url_read, \"html.parser\")\n",
    "    \n",
    "#~~~ Find all tables in the HTML reply PAYLOAD\n",
    "    edgar_request_soup_tables = edgar_request_soup.find_all('table')\n",
    "    \n",
    "#~~~ Although only one table is expected in the HTML reply PAYLOAD, it's always good practice to use a loop\n",
    "    for soup_table in edgar_request_soup_tables:\n",
    "#~~~ Find all the 'documentsbutton' tags (which have the target URL as a sub-tag) in the HTML reply PAYLOAD\n",
    "        href_list = edgar_request_soup.find_all('a', id='documentsbutton')\n",
    "#~~~ Loop through all the 'documentsbutton' tags found in the HTML reply PAYLOAD - This will vary with each request\n",
    "        for href in href_list:\n",
    "#~~~ Extract the URL from all the 'documentsbutton' tags found\n",
    "            doc_button_href = href['href']\n",
    "#~~~ Build a complete URL by adding the 'documentsbutton' tags (which only have the last part of the URL) to the 'sec_edgar_request_url_prefix' variable set above.            \n",
    "            sec_edgar_url = (sec_edgar_request_url_prefix + doc_button_href)\n",
    "#~~~ If the full URL is not already in the dictionary (we don't want duplicates), add it to the dictionary\n",
    "            if sec_edgar_url not in sec_edgar_doc_buttons_list:\n",
    "                sec_edgar_doc_buttons_list.append(sec_edgar_url)\n",
    "#~~~ Print the REQUEST and REPLY to and from Edgar to the console so the operator can review the execution statistics                         \n",
    "    print('Request Sent to SEC Edgar:', sec_edgar_request_url, '\\n')\n",
    "    print('Reply Received from SEC Edgar (Documents Button Target URL): ')\n",
    "    pprint.pprint(sec_edgar_doc_buttons_list)\n",
    "    print('_____________________________________________________________________ \\n')\n",
    "\n",
    "#~~~ Set the names for the Directory, Request File and Reply File Names\n",
    "doc_buttons_write_dir            = (output_os_dir_purpose_src_prefix + '/' \n",
    "                                    + sec_company_cik) \n",
    "\n",
    "doc_buttons_write_reply_file     = (output_os_dir_purpose_src_prefix + '/' \n",
    "                                    + sec_company_cik + '/' \n",
    "                                    + job_log_prefix + '_'\n",
    "                                    + job_log_suffix + '_Edgar.reply')\n",
    "\n",
    "doc_buttons_write_request_file   = (output_os_dir_purpose_src_prefix + '/' \n",
    "                                    + sec_company_cik + '/' \n",
    "                                    + job_log_prefix + '_'\n",
    "                                    + job_log_suffix + '_Edgar.request') \n",
    "\n",
    "#~~~ Create a sub-folder in the DMZ with the name equal to the variable used for 'sec_company_cik' if it doesn't exist\n",
    "if not os.path.exists(doc_buttons_write_dir):\n",
    "    print('PATH Not Found:     ', doc_buttons_write_dir)\n",
    "    try:\n",
    "        os.makedirs(doc_buttons_write_dir)\n",
    "        print(' ~ Directory Created:  ', doc_buttons_write_dir)\n",
    "    except:\n",
    "        print(' ~ ERROR: Creating Directory')\n",
    "else:\n",
    "    print('Directory Already Exists: ', doc_buttons_write_dir)\n",
    "\n",
    "#~~~ Populate the dictionary 'sec_edgar_request_metadata' to be used when writing the doc_buttons_write_request_file\n",
    "sec_edgar_request_metadata.append('DSB-3-LAB-1-A')\n",
    "sec_edgar_request_metadata.append(str(start_time))\n",
    "sec_edgar_request_metadata.append(job_log_suffix)\n",
    "\n",
    "sec_edgar_request_metadata.append(sec_company_cik)\n",
    "\n",
    "if not filing_type == ' ':\n",
    "    sec_edgar_request_metadata.append(filing_type)\n",
    "else:\n",
    "    sec_edgar_request_metadata.append('&type NONE')\n",
    "    \n",
    "if not until_date == ' ':\n",
    "    sec_edgar_request_metadata.append(until_date)\n",
    "else:\n",
    "    sec_edgar_request_metadata.append('&dateb NONE')\n",
    "    \n",
    "if not filing_count == ' ':\n",
    "    sec_edgar_request_metadata.append(filing_count)\n",
    "else:\n",
    "    sec_edgar_request_metadata.append('&count NONE')\n",
    "\n",
    "sec_edgar_request_metadata.append(sec_edgar_request_url)\n",
    "sec_edgar_request_metadata.append(storage_hierarchy_name)\n",
    "sec_edgar_request_metadata.append(doc_buttons_write_dir)\n",
    "sec_edgar_request_metadata.append(doc_buttons_write_request_file)\n",
    "sec_edgar_request_metadata.append(doc_buttons_write_reply_file)\n",
    "\n",
    "#~~~ #~~~ Storage Hierarchy Console Messages - So the operator knows which target directories were used to store any files created\n",
    "\n",
    "print('_____________________________________________________________________ \\n')\n",
    "print('Storage Hierarchy (' + storage_hierarchy_name + ') with Nodes Set As:')\n",
    "print(' ~ local_server_os   = ', local_server_os)\n",
    "print(' ~ ~ local_server_ip   = ', local_server_ip)\n",
    "print(' ~ ~ ~ local_root_drive   = ', local_root_drive)\n",
    "print(' ~ ~ ~ ~ dir_purpose        = ', dir_purpose)\n",
    "print(' ~ ~ ~ ~ ~ dir_source         = ', dir_source)\n",
    "print(' ~ ~ ~ ~ ~ ~ dir_src_sg1        = ', dir_src_sg1)\n",
    "print(' ~ ~ ~ ~ ~ ~ ~ dir_src_sg2        = ', dir_src_sg2)\n",
    "print(' ~ ~ ~ ~ ~ ~ ~ ~ JOB ID             = ', job_log_suffix)\n",
    "print('_____________________________________________________________________ \\n')\n",
    "\n",
    "#~~~ Print items in dictionary 'sec_edgar_request_metadata' to the console so the operator can review the request details\n",
    "print('sec_edgar_request_metadata:')\n",
    "pprint.pprint(sec_edgar_request_metadata)\n",
    "\n",
    "#~~~ Write dictionary 'sec_edgar_request_metadata' items to the doc_buttons_write_request_file\n",
    "with open(doc_buttons_write_request_file, 'w') as file_request:\n",
    "    for request_metadata_item in sec_edgar_request_metadata:\n",
    "        file_request.write(request_metadata_item)\n",
    "        file_request.write(' \\n')\n",
    "    \n",
    "file_request.close()\n",
    "\n",
    "#~~~ Write URL's collected above to the doc_buttons_write_reply_file\n",
    "with open(doc_buttons_write_reply_file, 'w') as f_reply:\n",
    "    for sec_edgar_doc_buttons_url in sec_edgar_doc_buttons_list:\n",
    "        f_reply.write(sec_edgar_doc_buttons_url)\n",
    "        f_reply.write(' \\n')\n",
    "        \n",
    "f_reply.close()\n",
    "\n",
    "#### \n",
    "####----- Python Routine Termination\n",
    "#### \n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print (' \\n')\n",
    "print ('<-<-<-<-<-<-<-<-<-<----  E X E C U T I O N   C O M P L E T E  ---->->->->->->->->->->')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('>>>>>               R O U T I N E ................. DSB-3-LAB-1-A               <<<<<')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('>>>>>         End Time: ', end_time, '                            <<<<<')\n",
    "print ('>>>>>       Start Time: ', start_time, '                            <<<<<')\n",
    "print ('>>>>>                    ~~~~~~~~~~~~~~~~~~~~~~~~~~', '                            <<<<<')\n",
    "print ('>>>>>   Execution Time:             ', execution_time, '                            <<<<<')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('<-<-<-<-<-<-<-<-<-<----  E X E C U T I O N   C O M P L E T E  ---->->->->->->->->->->')\n",
    "print (' ')\n",
    "print('_____________________________________________________________________ \\n')\n",
    "#\n",
    "#\n",
    "#\n",
    "####----- Chameleon Metadata Student Lab DSB-3-LAB-1-B\n",
    "#\n",
    "import datetime\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print ('<-<-<----- C H A M E L E O N   M E T A D A T A   P Y T H O N   S C R I P T  ---->->->')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('>>>>>               ROUTINE NAME:                   DSB-3-LAB-1-B               <<<<<')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('>>>>>               Start Time:       ', start_time, '              <<<<<')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('<-<-<-<-<-<-<-<-<-------  E X E C U T I O N   B E G I N S  ------->->->->->->->->->-> \\n')\n",
    "\n",
    "#### \n",
    "####----- Python Environment Division\n",
    "#### \n",
    "\n",
    "import os\n",
    "import pprint\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#### \n",
    "####----- Working Storage Section (WSS)\n",
    "#### \n",
    "\n",
    "#~~~ DEFINITION: A \"Chameleon Metadata Protocol\" is a collection of routines executed together and in sequence as a workflow.\n",
    "#~~~\n",
    "#~~~ 'job_log_suffix' is used so a common \"JOB Prefix\" can be shared by all routines run together as a protocol.\n",
    "#~~~ \n",
    "#~~~ However, if that variable was NOT set by a past step, this script will ABEND. That's why we try a \"DUMMY SET\" to see if it exists\n",
    "\n",
    "try:\n",
    "    wss_check_protocol_datetime_exists    = job_log_suffix\n",
    "except:\n",
    "    job_log_suffix                        = ' '\n",
    "    \n",
    "#~~~ Now the variable 'job_log_suffix' may be set regardless of its state before this script executes\n",
    "if not job_log_suffix       == ' ':\n",
    "    print('job_log_suffix IN-MEMORY from past step:', job_log_suffix)\n",
    "    print(' ~ Setting JOB ID (job_log_suffix) to: ', job_log_suffix, ' \\n')\n",
    "else:\n",
    "    job_log_suffix          = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "    print('job_log_suffix NOT SET by previous step - Initializing Now As: ', job_log_suffix)\n",
    "    print(' ~ Setting JOB ID (job_log_suffix) to: ', job_log_suffix, ' \\n')\n",
    "\n",
    "#~~~ #~~~ Chameleon Metadata Storage Hierarchy Variables\n",
    "\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<storage_hierarchy_name            = 'windows-dsb-3-lab-1-b'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<job_log_prefix                    = 'DSB-3-LAB-1-A'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<job_log_suffix                    = job_log_suffix\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<local_server_os                   = 'Windows'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<local_server_ip                   = '192.168.1.114'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<local_root_drive                  = 'c'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<dir_purpose                       = 'dmz-ftp'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<dir_source                        = 'sec-edgar'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<dir_src_sg1                        = 'cik'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<dir_src_sg2                        = 'secNum'\n",
    "\n",
    "####\n",
    "##----------------- WSS Static Values Variables\n",
    "#### \n",
    "\n",
    "##------------------------ WSS SEC Edgar Manditory\n",
    "#~~~ \n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<sec_company_cik                = '0001631574'      #-- &CIK URL variable - Make sure to include leading zeros\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<sec_edgar_request_url_prefix      = 'https://www.sec.gov'\n",
    "\n",
    "##------------------------ WSS SEC Edgar Optional\n",
    "\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<filing_type                    = '10-K'                  #-- &type URL variable (Filter on type of fileing)\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<until_date                     = ' '    #-- Format: 'YYYYMMDD'   #-- &dateb URL variable (Filter on maximum filing date requested)\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<filing_count                   = ' '    #-- Format: '99'         #-- &count URL variable (Limit the number of filings to return)\n",
    "\n",
    "#~~~ #~~~ SEC Edgar-related URL's and Prefixes\n",
    "\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<sec_edgar_request_url_prefix      = 'https://www.sec.gov'\n",
    "#--- >>>>> COMMENTED OUT FOR LAB <<<<<sec_edgar_document_request_prefix = 'Archives/edgar/data'\n",
    "\n",
    "sec_edgar_document_request_url    = ' '\n",
    "\n",
    "#~~~ #~~~ Windows Directories & File Variables (Note alignment of variable names to the storage hierarchy being used)\n",
    "\n",
    "output_os_dir_purpose_src_prefix  = ' '\n",
    "output_os_dir_src_sg2_job         = ' '\n",
    "\n",
    "output_os_hadoop_mkdir_cmds_file  = ' '\n",
    "output_os_hadoop_copy_cmds_file   = ' '\n",
    "\n",
    "output_os_replyDocButtons_file    = ' '\n",
    "output_os_replyDocsMetadata_file  = ' '\n",
    "output_os_replyDocsURL_file       = ' '\n",
    "output_os_replyDocument_file      = ' '\n",
    "\n",
    "#~~~ #~~~ Hadoop Directories & File Variables\n",
    "\n",
    "hadoop_target_dir_purpose         = dir_purpose\n",
    "\n",
    "hadoop_purpose_source1_prefix     = ' '\n",
    "hadoop_mkdir_cmd_src1_prefix      = ' '\n",
    "hadoop_mkdir_src_sg1_prefix       = ' '\n",
    "hadoop_mkdir_src_sg2              = ' '\n",
    "\n",
    "hadoop_copy_dir_src_sg2_job       = ' '\n",
    "hadoop_copyFromLocal_prefix       = ' '\n",
    "\n",
    "#~~~ #~~~ Filing and Document Metadata Variables\n",
    "\n",
    "wss_enumerated_edgar_info         = ' '\n",
    "wss_edgar_filing_date             = ' '\n",
    "wss_edgar_filing_accepted         = ' '\n",
    "wss_edgar_filing_documents        = ' '\n",
    "wss_edgar_filing_period_of_rpt    = ' '\n",
    "\n",
    "metadata_document_secNum          = ' '\n",
    "metadata_document_description     = ' '\n",
    "metadata_document_name            = ' '\n",
    "metadata_document_size            = ' ' \n",
    "\n",
    "wss_doc_description               = ' '\n",
    "wss_document_size                 = ' ' \n",
    "\n",
    "wss_hadoop_copy_command           = ' '\n",
    "\n",
    "edgar_form_page_metadata_count    = 0\n",
    "edgar_secNum_count                = 0\n",
    "\n",
    "####\n",
    "##----------------- Dictionaries, Data Frames, Files Etc.\n",
    "#### \n",
    "\n",
    "#~~~ #~~~ Initialize all the Python dictionaries\n",
    "#~~~ #~~~ \n",
    "#~~~ #~~~ #~~~ Dictionary 'sec_edgar_doc_buttons_list' was already set during DSB-3-LAB-1-A. \n",
    "#~~~ #~~~ #~~~ To run stand-alone, LAB-1-B could also READ the 'Edgar.reply' file created when LAB-1-A was executed  \n",
    "#~~~ #~~~ #~~~ However, in a Learning Lab environment, it's less complex to have students execute LAB-1-A first\n",
    "\n",
    "# sec_edgar_doc_buttons_list        = []\n",
    "\n",
    "reply_metadata_list               = []\n",
    "document_metadata_list            = []\n",
    "\n",
    "button_page_metadata              = []\n",
    "\n",
    "sec_edgar_doc_infoHeads           = []\n",
    "sec_edgar_doc_infos               = []\n",
    "sec_edgar_document_urls           = []\n",
    "\n",
    "hadoop_mkdir_command_list         = []\n",
    "hadoop_copy_command_list          = []\n",
    "\n",
    "#### \n",
    "####----- Python Procedure Division\n",
    "#### \n",
    "\n",
    "#~~~ Build the directories which will be used to store the outcome (reply data and job logs) of this Python routine\n",
    "\n",
    "hadoop_purpose_source1_prefix     = (dir_purpose + '/'\n",
    "                                     + dir_source)\n",
    "\n",
    "output_os_dir_purpose_src_prefix  = (local_root_drive + ':' + '/'\n",
    "                                     + hadoop_purpose_source1_prefix)\n",
    "\n",
    "output_os_directory_cik_prefix    = (local_root_drive  + ':' + '/'\n",
    "                                     + hadoop_purpose_source1_prefix + '/'\n",
    "                                     + sec_company_cik)\n",
    "\n",
    "hadoop_copyFromLocal_prefix       = ('bin/hadoop fs -copyFromLocal \"//' \n",
    "                                     + local_server_ip + '/'\n",
    "                                     + local_root_drive + '/' \n",
    "                                     + hadoop_purpose_source1_prefix + '/' ) \n",
    "\n",
    "hadoop_mkdir_cmd_src1_prefix      = ('bin/hadoop fs -mkdir /'\n",
    "                                     + hadoop_purpose_source1_prefix)\n",
    "    \n",
    "output_os_replyDocButtons_file    = (output_os_directory_cik_prefix + '/'\n",
    "                                     + job_log_prefix + '_'\n",
    "                                     + job_log_suffix + '_replyDocButtons.metadata')\n",
    "\n",
    "output_os_hadoop_mkdir_cmds_file  = (output_os_directory_cik_prefix + '/'\n",
    "                                     + job_log_prefix + '_'\n",
    "                                     + job_log_suffix + '_mkdir.hadoop')\n",
    "\n",
    "#~~~ IMPORTANT HEADS-UP: Python dictionary 'sec_edgar_doc_buttons_list', already in memory after executing LAB-1-A, is used below.\n",
    "#~~~\n",
    "#~~~ In 'real life', Protocol Scripts should READ the files created by previous steps rather than use in-memory dictionaries. \n",
    "#~~~ That way, after each step runs, its files act quiesce points in case any subsequent steps in the Protocol's workflow ABEND.\n",
    "#~~~ But this is just a Student Lab, so we'll read the 'Documents Button' URL's from the Python dictionary to make things simpler.\n",
    "\n",
    "for sec_edgar_doc_button_url in sec_edgar_doc_buttons_list:\n",
    "    with urllib.request.urlopen(sec_edgar_doc_button_url) as edgar_request_url:\n",
    "        edgar_request_url_read = edgar_request_url.read()\n",
    "\n",
    "#~~~ Each time a new Documents Button URL is visited, reinitialize the form-specific Python dictionaries\n",
    "    sec_edgar_doc_infoHeads         = []\n",
    "    sec_edgar_doc_infos             = []\n",
    "\n",
    "#~~~ \n",
    "#~~~ BeautifulSoup Configuration\n",
    "\n",
    "    soup_edgar_request              = BeautifulSoup(edgar_request_url_read, \"html.parser\")\n",
    "    \n",
    "    soup_edgar_companyInfo          = soup_edgar_request.find_all('div', {\"class\": \"companyInfo\"})\n",
    "    soup_edgar_companyNames         = soup_edgar_request.find_all('span', {\"class\": \"companyName\"})\n",
    "    soup_edgar_identInfos           = soup_edgar_request.find_all('p', {\"class\": \"identInfo\"})\n",
    "    soup_edgar_secNums              = soup_edgar_request.find_all('div', id='secNum')\n",
    "    soup_edgar_formNames            = soup_edgar_request.find_all('div', id='formName')\n",
    "    soup_edgar_infoHeads            = soup_edgar_request.find_all('div', {\"class\": \"infoHead\"})\n",
    "    soup_edgar_infos                = soup_edgar_request.find_all('div', {\"class\": \"info\"})\n",
    "    soup_document_format_files      = soup_edgar_request.find_all('div', {\"summary\": \"Document Format Files\"})\n",
    "    soup_edgar_request_tables       = soup_edgar_request.find_all('table')\n",
    "\n",
    "#~~~ Get the company name from the Edgar Documents Button target URL which has a list of documents making up a 'form' (10-K, 13-F, etc.)\n",
    "    for edgar_companyName in soup_edgar_companyNames:\n",
    "        edgar_companyName_text      = edgar_companyName.text\n",
    "        edgar_companyName_text      = edgar_companyName_text.replace('\\n', '') \n",
    "        edgar_companyName_text      = edgar_companyName_text.replace(' (see all company filings)', '')\n",
    "\n",
    "#~~~ Get the SEC Accession Number, which is an identifier given to each individual form filing (i.e. 10-K's get different secNum each year they are filed)\n",
    "    for edgar_secNum in soup_edgar_secNums:   \n",
    "        edgar_secNum                      = edgar_secNum\n",
    "        edgar_secNum_only                 = (edgar_secNum.text[19:])\n",
    "        \n",
    "        edgar_secNum_only                 = edgar_secNum_only.replace(' ', '')\n",
    "        edgar_secNum_only                 = edgar_secNum_only.replace('\\n', '')\n",
    "        \n",
    "        edgar_secNum_clean                = edgar_secNum_only.replace('-', '')\n",
    "\n",
    "        \n",
    "#~~~ Because the 'secNum' is used as a sub-directory name whilst building target directories, the directory names are rebuilt for each form\n",
    "\n",
    "        output_os_dir_src_sg2_job         = (output_os_dir_purpose_src_prefix + '/'\n",
    "                                             + sec_company_cik + '/'\n",
    "                                             + edgar_secNum_only + '/'\n",
    "                                             + job_log_suffix)\n",
    "        \n",
    "        hadoop_target_dir_purpose         = (hadoop_purpose_source1_prefix + '/'\n",
    "                                             + sec_company_cik + '/'\n",
    "                                             + edgar_secNum_only + '/'\n",
    "                                             + job_log_suffix)\n",
    "        \n",
    "#~~~ Build the full Hadoop 'mkdir' and 'copyFromLocal' HDFS commands\n",
    "\n",
    "        hadoop_mkdir_src_sg2              = (hadoop_mkdir_cmd_src1_prefix + '/'\n",
    "                                             + sec_company_cik + '/'\n",
    "                                             + edgar_secNum_only + '/'\n",
    "                                             + job_log_suffix)\n",
    "        \n",
    "        hadoop_copy_dir_src_sg2_job       = (hadoop_copyFromLocal_prefix\n",
    "                                             + sec_company_cik + '/'\n",
    "                                             + edgar_secNum_only + '/'\n",
    "                                             + job_log_suffix +'\"' + ' ') #~~~ Don't forget the trailing blank space             \n",
    "\n",
    "#~~~ Once the target JOB directory name is known, build a directory with that name if it doesn't already exists\n",
    "\n",
    "#~~~ REMEMBER: By storing in the 'job_log-suffix' sub-directory (a child of the secNum diretory), we can do many runs per secNUM.\n",
    "\n",
    "        if not os.path.exists(output_os_dir_src_sg2_job):\n",
    "            print('PATH Not Found:     ', output_os_dir_src_sg2_job)\n",
    "            try:\n",
    "                os.makedirs(output_os_dir_src_sg2_job)\n",
    "                print(' ~ Directory Created:  ', output_os_dir_src_sg2_job, ' \\n')\n",
    "            except:\n",
    "                print('-+-> ERROR: Creating Directory: ', output_os_dir_src_sg2_job)                \n",
    "        else:\n",
    "            print('Directory Already Exists: ', output_os_dir_src_sg2_job, ' \\n')\n",
    "\n",
    "#~~~ These next BeautifulSoup bits use multiple BeautifulSoup config's (defined above) to collect the form page's headers and details\n",
    "\n",
    "#~~~ the formName (10-K, 13-F, etc.)\n",
    "    for edgar_formName in soup_edgar_formNames:\n",
    "        edgar_formName_only           = (edgar_formName.text)\n",
    "        edgar_formName_only           = edgar_formName_only.replace('\\n', '')\n",
    "\n",
    "#~~~ the headers from the page containing values like 'filing_date', 'SEC Accession Number', 'Period of Report', etc.\n",
    "    for edgar_infoHead in soup_edgar_infoHeads:\n",
    "        edgar_infoHead_only           = (edgar_infoHead.text)\n",
    "\n",
    "#~~~ remove the line-break from each header because errant HTML tags can mess with Natural Language Processing (NLP) tasks\n",
    "        edgar_infoHead_only           = edgar_infoHead_only.replace('\\n', '')\n",
    "        sec_edgar_doc_infoHeads.append(edgar_infoHead_only)\n",
    "\n",
    "#~~~ Now, get each page headers' text values\n",
    "    for edgar_info in soup_edgar_infos:\n",
    "        sec_edgar_doc_infos.append(edgar_info.text)\n",
    "        \n",
    "#~~~ Next, we loop through all the 'Infos' and align their text values with their corrisponding page headers (infoHeads)            \n",
    "    for i_infoHead, enumerated_doc_infoHead in enumerate(sec_edgar_doc_infoHeads):\n",
    "        enumerated_edgar_info         = sec_edgar_doc_infos[i_infoHead] \n",
    "\n",
    "#~~~ #~~~ REMEMBER: We first loop through each URL for each Documents Button. Then we'll lopp though each dataset at each page\n",
    "\n",
    "#~~~ Store all the metadata from each Form Page pointed to by each Documents Button on the initial reply from Edgar (LAB-1-A)\n",
    "    reply_metadata_list.append(sec_company_cik + ', ' \n",
    "                               + edgar_companyName_text + ', '  \n",
    "                               + edgar_secNum_only + ', ' \n",
    "                               + edgar_secNum_clean + ', ' \n",
    "                               + job_log_suffix + ', '\n",
    "                               + edgar_formName_only + ', ' \n",
    "                               + sec_edgar_doc_infos[0]  + ', ' \n",
    "                               + sec_edgar_doc_infos[1] + ', ' \n",
    "                               + sec_edgar_doc_infos[2]  + ', ' \n",
    "                               + sec_edgar_doc_infos[3]  + ', ' \n",
    "                               + storage_hierarchy_name + ','\n",
    "                               + output_os_dir_src_sg2_job + ', '\n",
    "                               + output_os_replyDocButtons_file + ', '\n",
    "                               + hadoop_target_dir_purpose + ', '\n",
    "                               + hadoop_mkdir_src_sg2 + ', '\n",
    "                               + sec_edgar_doc_button_url)\n",
    "    \n",
    "#~~~ Store the full Hadoop 'mkdir' HDFS commands in a Python dictionary so we can write them all to a file at the end of the run.\n",
    "\n",
    "    if hadoop_mkdir_src_sg2 not in hadoop_mkdir_command_list:    \n",
    "        hadoop_mkdir_command_list.append(hadoop_mkdir_src_sg2)\n",
    "\n",
    "#~~~ Now, parse the HTML table on each 'Documents Button' page for metadata about each of the many datasets per form (per 10-K, etc.)\n",
    "\n",
    "#~~~ HEADS UP: This bit is a little tricky.  We have a table with 5 columns and an unknown number of rows.\n",
    "#~~~           But, since we know we have 5 columns, we also know each time the counter reaches 6, it means a new table row & document\n",
    "#~~~           So, when the counter reaches 5, we store the document metadata, build '.metadata' file names, add to Python\n",
    "#~~~           dictionaries and write the actual file to the dircotory whose name was pre-built based on the HTML details \n",
    "#~~~           collected for each dataset before the WRITE occurs\n",
    "\n",
    "    for edgar_request_table in soup_edgar_request_tables:\n",
    "#~~~ Reset this counter for each new Documents Button page (aka Form page) - 1 Form page per Button URL & many datasets per Form\n",
    "        edgar_form_page_metadata_count             = 0\n",
    "\n",
    "        for td in edgar_request_table.findAll(\"td\"):            \n",
    "            edgar_form_page_metadata_count = edgar_form_page_metadata_count + 1\n",
    "\n",
    "#~~~ When the counter reaches 6, it means the code is reading a new row for a new dataset. So, the counter is initialized again to = 1 for\n",
    "            if edgar_form_page_metadata_count      >= 6:\n",
    "                edgar_form_page_metadata_count     = 1\n",
    "                \n",
    "#~~~ Sequence Number (the first table column) doesn't\n",
    "#~~~ I'll ignore it as it's not of much value because we want ALL files & Edgar DOES NOT assign a 'seq' to the .txt files.\n",
    "                \n",
    "                td_findNext_string                 = (td.findNext(text=True))\n",
    "            else:\n",
    "                td_findNext_string                 = (td.findNext(text=True))\n",
    "    \n",
    "#~~~ Document Description                \n",
    "                if (edgar_form_page_metadata_count == 2):\n",
    "                    metadata_document_description  = td_findNext_string    \n",
    "\n",
    "                    wss_doc_description            = metadata_document_description\n",
    "                \n",
    "#~~~ Document Name                \n",
    "                if (edgar_form_page_metadata_count == 3):\n",
    "                    metadata_document_name = td_findNext_string\n",
    "            \n",
    "#~~~ Document Size\n",
    "#~~~ AND - edgar_form_page_metadata_count == 5 means this is the last table column for this row. So, there is extra work to do here.\n",
    "\n",
    "                if (edgar_form_page_metadata_count == 5):\n",
    "                    metadata_document_size = td_findNext_string \n",
    "                    \n",
    "                    wss_document_size           = metadata_document_size\n",
    "                    \n",
    "                    wss_hadoop_copy_command     = (hadoop_copy_dir_src_sg2_job + hadoop_target_dir_purpose)\n",
    "                                        \n",
    "                    if  wss_hadoop_copy_command not in hadoop_copy_command_list:\n",
    "                        hadoop_copy_command_list.append(wss_hadoop_copy_command)\n",
    "        \n",
    "                    metadata_document_secNum = (sec_company_cik  + ', '  \n",
    "                                                + edgar_secNum_only  + ', '  \n",
    "                                                + edgar_secNum_clean + ', '\n",
    "                                                + metadata_document_name + ', '\n",
    "                                                + wss_doc_description + ', '    \n",
    "                                                + wss_document_size + ', '\n",
    "                                                + job_log_suffix + ', '\n",
    "                                                + storage_hierarchy_name + ','\n",
    "                                                + output_os_dir_src_sg2_job + ', '\n",
    "                                                + hadoop_target_dir_purpose + ', '\n",
    "                                                + wss_hadoop_copy_command + ', '\n",
    "                                                + sec_edgar_doc_button_url)\n",
    "                    \n",
    "                    output_os_replyDocsMetadata_file  = (output_os_directory_cik_prefix + '/'\n",
    "                                                         + job_log_prefix + '_'\n",
    "                                                         + job_log_suffix + '_replyDocuments.metadata')\n",
    "                    \n",
    "                    output_os_replyDocsURL_file       = (output_os_directory_cik_prefix + '/'\n",
    "                                                         + job_log_prefix + '_'\n",
    "                                                         + job_log_suffix + '_replyDocsURL.metadata')\n",
    "                    \n",
    "                    if metadata_document_secNum not in document_metadata_list:\n",
    "                        document_metadata_list.append(metadata_document_secNum)\n",
    "                        \n",
    "                    \n",
    "                    output_os_hadoop_copy_cmds_file   = (output_os_directory_cik_prefix + '/'\n",
    "                                                         + job_log_prefix + '_'\n",
    "                                                         + job_log_suffix + '_copyFromLocal.hadoop')\n",
    "    \n",
    "                    sec_edgar_document_request_url    = (sec_edgar_request_url_prefix + '/'\n",
    "                                                         + sec_edgar_document_request_prefix + '/'\n",
    "                                                         + sec_company_cik + '/'\n",
    "                                                         + edgar_secNum_clean + '/'\n",
    "                                                         + metadata_document_name)\n",
    "            \n",
    "                    output_os_replyDocument_file      = (output_os_dir_src_sg2_job + '/'\n",
    "                                                         + metadata_document_name)\n",
    "                \n",
    "                    if sec_edgar_document_request_url not in sec_edgar_document_urls:\n",
    "                        sec_edgar_document_urls.append(sec_edgar_document_request_url)\n",
    "                    \n",
    "                        requestDocURL  = requests.get(sec_edgar_document_request_url, allow_redirects=True)\n",
    "                        \n",
    "                        with open(output_os_replyDocument_file, 'wb')as docfile_request:\n",
    "                            docfile_request.write(requestDocURL.content)\n",
    "\n",
    "                            docfile_request.close()\n",
    "\n",
    "#~~~ This last set of paragraphs WRITEs the five (5) Python Dictionaries out to flat-files with .meatadata & .hadoop file extensions\n",
    "#~~~ It's alway best to limit the number of open flat-files at any given time - I try and have only one file open at a time.\n",
    "#~~~ And, by reusing the variable 'file_request', I'm sure to remember to CLOSE each file - If not, the next file's OPEN will ABEND\n",
    "\n",
    "with open(output_os_replyDocButtons_file, 'w') as file_request:\n",
    "    for reply_metadata_item in reply_metadata_list:\n",
    "        file_request.write(reply_metadata_item)\n",
    "        file_request.write(' \\n')\n",
    "        \n",
    "file_request.close()\n",
    "        \n",
    "with open(output_os_replyDocsMetadata_file, 'w') as file_request:\n",
    "    for document_metadata_item in document_metadata_list:\n",
    "        file_request.write(document_metadata_item)\n",
    "        file_request.write(' \\n')\n",
    "    \n",
    "file_request.close()\n",
    "\n",
    "with open(output_os_replyDocsURL_file, 'w') as file_request:\n",
    "    for sec_edgar_document_item in sec_edgar_document_urls:\n",
    "        file_request.write(sec_edgar_document_item)\n",
    "        file_request.write(' \\n')\n",
    "    \n",
    "file_request.close()\n",
    "\n",
    "with open(output_os_hadoop_mkdir_cmds_file, 'w') as file_request:\n",
    "    for hadoop_mkdir_command_item in hadoop_mkdir_command_list:\n",
    "        file_request.write(hadoop_mkdir_command_item)\n",
    "        file_request.write(' \\n')\n",
    "    \n",
    "file_request.close()\n",
    "\n",
    "with open(output_os_hadoop_copy_cmds_file, 'w') as file_request:\n",
    "    for hadoop_copy_command_item in hadoop_copy_command_list:\n",
    "        file_request.write(hadoop_copy_command_item)\n",
    "        file_request.write(' \\n')\n",
    "    \n",
    "file_request.close()\n",
    "\n",
    "#~~~ #~~~ Storage Hierarchy Console Messages - So the operator knows which target directories will be used to store any files created\n",
    "\n",
    "print('_____________________________________________________________________ \\n')\n",
    "print('Storage Hierarchy (' + storage_hierarchy_name + ') with Nodes Set As:')\n",
    "print(' ~ local_server_os   = ', local_server_os)\n",
    "print(' ~ ~ local_server_ip   = ', local_server_ip)\n",
    "print(' ~ ~ ~ local_root_drive   = ', local_root_drive)\n",
    "print(' ~ ~ ~ ~ dir_purpose        = ', dir_purpose)\n",
    "print(' ~ ~ ~ ~ ~ dir_source         = ', dir_source)\n",
    "print(' ~ ~ ~ ~ ~ ~ dir_src_sg1        = ', dir_src_sg1)\n",
    "print(' ~ ~ ~ ~ ~ ~ ~ dir_src_sg2        = ', dir_src_sg2)\n",
    "print(' ~ ~ ~ ~ ~ ~ ~ ~ JOB ID             = ', job_log_suffix)\n",
    "print('_____________________________________________________________________ \\n')\n",
    "\n",
    "#~~~ DEBUG print('oooooooooooooooooooooooooooooooooooooo')\n",
    "#~~~ DEBUG print('SEC Edgar Reply: Target URL List Member Metadata (one collection for each Documents button in the reply webpage): ')\n",
    "#~~~ DEBUG print('oooooooooooooooooooooooooooooooooooooo \\n')\n",
    "#~~~ DEBUG for request_metadata_item in reply_metadata_list:\n",
    "#~~~ DEBUG     pprint.pprint(request_metadata_item) \n",
    "#~~~ DEBUG     print(' \\n')\n",
    "#~~~ DEBUG print('_____________________________________________________________________ \\n')\n",
    "\n",
    "#~~~ DEBUG print(' \\n')\n",
    "#~~~ DEBUG print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')\n",
    "#~~~ DEBUG print('Edgar Dataset Metadata (one for each Dataset listed @ each Documents button URL in the reply webpage): ')\n",
    "#~~~ DEBUG print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')       \n",
    "#~~~ DEBUG for document_metadata_item in document_metadata_list: \n",
    "#~~~ DEBUG     pprint.pprint(document_metadata_item)   \n",
    "#~~~ DEBUG     print(' \\n')\n",
    "#~~~ DEBUG print('_____________________________________________________________________ \\n')\n",
    "#~~~ DEBUG \n",
    "#~~~ DEBUG print('wwwwwwwwwwwwwwwwwwwwwwwwwwwwwww')\n",
    "#~~~ DEBUG print('Dataset URLs by secNum (one for each Dataset listed @ each Documents button URL in the reply webpage): ')\n",
    "#~~~ DEBUG print('wwwwwwwwwwwwwwwwwwwwwwwwwwwwwww')\n",
    "#~~~ DEBUG pprint.pprint(sec_edgar_document_urls)\n",
    "#~~~ DEBUG print('_____________________________________________________________________ \\n')\n",
    "\n",
    "#### \n",
    "####----- Python Routine Termination\n",
    "#### \n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print (' \\n')\n",
    "print ('<-<-<-<-<-<-<-<-<-<----  E X E C U T I O N   C O M P L E T E  ---->->->->->->->->->->')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('>>>>>               R O U T I N E ................. DSB-3-LAB-1-B               <<<<<')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('>>>>>         End Time: ', end_time, '                            <<<<<')\n",
    "print ('>>>>>       Start Time: ', start_time, '                            <<<<<')\n",
    "print ('>>>>>                    ~~~~~~~~~~~~~~~~~~~~~~~~~~', '                            <<<<<')\n",
    "print ('>>>>>   Execution Time:             ', execution_time, '                            <<<<<')\n",
    "print ('>>>>>                                                                           <<<<<')\n",
    "print ('<-<-<-<-<-<-<-<-<-<----  E X E C U T I O N   C O M P L E T E  ---->->->->->->->->->->')\n",
    "# \n",
    "#############################################################################################################################\n",
    "#---------------------------------------------      Chameleon-DSB-3-LAB-1-B     ---------------------------------------------\n",
    "#--------------------------   © 2018 Dynamic Database Support Systems and ChameleonMetadata.com    -------------------------- \n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------------------------   MIT Creative Commons License   ---------------------------------------------\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated  \n",
    "# documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation \n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and \n",
    "# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# 1) The above copyright notice & this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "#\n",
    "# 2) The Software is provided \"as is\", without warranty of any kind, express or implied, including but not limited to \n",
    "#    the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors \n",
    "#    or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or \n",
    "#    otherwise, arising from, out of or in connection with the software or the use or other dealings in the Software.\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
