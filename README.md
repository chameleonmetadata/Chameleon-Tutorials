# Protocol-DSB-3-LAB-1-AB (Jupyter and .py versions available)

Back in tutorial DSB-3-1 (see it here) we walked through the creation of nine Capabilities we were looking to provide at the haircare products company Curl Up & Dye. Student Lab, DSB-3-LAB-1-A, was the first Hands-On Student Labs for the series and its expected learning outcome was to enable students to create a URL to request filings from the SEC's Edgar system. The reply consisted of a list of URL's for the SEC filings which met the filter criteria we provided (Python code for that is available at my GitHub site: LAB-1-A).

For this lab the code from LAB-1-A and LAB-1-B has been combined into a single set of code which Chameleon Metadata refers to as a Python Protocol. In the 'real world' Python programming, you can think of LAB-1-A and LAB-1-B as Python Methods and a Python Protocol as a Python Class. But, I have simply 'stitched the two pieces of code together' instead of coding it as a Class with Methods to make it less complex for students new to Python coding.

The learning outcome here is to have the students use the output from LAB-1-A as input and walk-through and write the BeautifulSoup code required to do the following:

Re-execute the LAB-1-A code and store the reply records in a Python dictionary to be used by LAB-1-B to locate and download all the datasets associated with a given SEC filing form (i.e. 10-K, 13F, etc.)
Build the directories needed to store the many datasets (a single 10-K returns multiple datasets) related to each given filing. Based on the following storage hierarchy:
The Root Drive and names provided at runtime for user-defined, high-level directory names. The Python code current at our GitHub site uses the following high-level naming scheme: c:/dmz-ftp/sec-edgar/
A company's unique CIK (Central Index Key) which the SEC assigns to each company;
The secNum (SEC Accession Number) which is assigned to each filing. When a company files a 10-K for 2016 and 2017, each year's 10-K will get its own unique secNum
A unique JOB ID which is generated by the system timestamp which allows the same job the be executed multiple times and still have each run's output segregated into its own folder under the directory nodes discussed in items 1-3 above
Generate the Hadoop mkdir commands to create HDFS directories aligned to and named the same as the storage hierarchy defined above - including JOB ID.
Use BeautifulSoup to visit each page, as pointed to by the 'Documents' buttons (Document Button Page) listed in the reply received from Edgar when LAB-1-A was executed.
For each Documents Button Page, we will create a directory in both Windows and Hadoop which whose name is aligned to the secNum assigned to each filing by the SEC. Then, we will add a sub-directory named for the JOB ID assigned at the start of the execution.
Whilst visiting each Documents Button Page, we need to collect all the URL's for the multiple documents listed there. And remember, each form (i.e. 10-K) there are many files at its given Documents Button page URL. These include HTML, JPG, XML and TXT files - some or all of which may all be present for any single filing.
Visit each URL for each dataset on each Documents Button Page and download all the datasets listed there into the directories which were built in an earlier step.
Generate the Hadoop copyFromLocal commands to copy all the datasets from the step immediately before this one into the HDFS directories which were built in an earlier step.
There are four types of datasets created when this Protocol script (with LAB-1-A and LAB-1-B combined) is executed:

_Edgar.request file: Contains metadata about the script, the URL sent to Edgar and the directories and file names which will be used for the Request sent to and the Reply received from Edgar.
_Edgar.reply file: Contains the URL's for all the 'Documents' buttons present on the web page you would see if you just type the Request URL into your browser. With the LAB-1-B Request URL we built in LAB-1-A, we filtered for '10-K' filings which include forms 10-K and 10-K/A.
.metadata files: Contains metadata values used by Chameleon Metadata and will be covered later in the DSB-3 tutorial series.
.hadoop files: Contains the HDFS mkdir commands to make the HDFS directories into which the Edgar datasets will be loaded and the copyFromLocal commands which will copy all the downloaded datasets into HDFS.DSB-3-LAB-1-AB
In a hurry and don't want to read all these words before you get to the actual Python code? No Problem.

You can download the Python/BeautifulSoup code here: GitHub Site for Protocol-DSB-3-LAB-1-AB (Jupyter and .py versions available)

OK, back to this student lab. Back in tutorial DSB-3-1 (see it here) we walked through the creation of nine Capabilities we were looking to provide at the haircare products company Curl Up & Dye. Student Lab, DSB-3-LAB-1-A, was the first Hands-On Student Labs for the series and its expected learning outcome was to enable students to create a URL to request filings from the SEC's Edgar system. The reply consisted of a list of URL's for the SEC filings which met the filter criteria we provided (Python code for that is available at my GitHub site: LAB-1-A).

For this lab the code from LAB-1-A and LAB-1-B has been combined into a single set of code which Chameleon Metadata refers to as a Python Protocol. In the 'real world' Python programming, you can think of LAB-1-A and LAB-1-B as Python Methods and a Python Protocol as a Python Class. But, I have simply 'stitched the two pieces of code together' instead of coding it as a Class with Methods to make it less complex for students new to Python coding.

The learning outcome here is to have the students use the output from LAB-1-A as input and walk-through and write the BeautifulSoup code required to do the following:

Re-execute the LAB-1-A code and store the reply records in a Python dictionary to be used by LAB-1-B to locate and download all the datasets associated with a given SEC filing form (i.e. 10-K, 13F, etc.)
Build the directories needed to store the many datasets (a single 10-K returns multiple datasets) related to each given filing. Based on the following storage hierarchy:
The Root Drive and names provided at runtime for user-defined, high-level directory names. The Python code current at our GitHub site uses the following high-level naming scheme: c:/dmz-ftp/sec-edgar/
A company's unique CIK (Central Index Key) which the SEC assigns to each company;
The secNum (SEC Accession Number) which is assigned to each filing. When a company files a 10-K for 2016 and 2017, each year's 10-K will get its own unique secNum
A unique JOB ID which is generated by the system timestamp which allows the same job the be executed multiple times and still have each run's output segregated into its own folder under the directory nodes discussed in items 1-3 above
Generate the Hadoop mkdir commands to create HDFS directories aligned to and named the same as the storage hierarchy defined above - including JOB ID.
Use BeautifulSoup to visit each page, as pointed to by the 'Documents' buttons (Document Button Page) listed in the reply received from Edgar when LAB-1-A was executed.
For each Documents Button Page, we will create a directory in both Windows and Hadoop which whose name is aligned to the secNum assigned to each filing by the SEC. Then, we will add a sub-directory named for the JOB ID assigned at the start of the execution.
Whilst visiting each Documents Button Page, we need to collect all the URL's for the multiple documents listed there. And remember, each form (i.e. 10-K) there are many files at its given Documents Button page URL. These include HTML, JPG, XML and TXT files - some or all of which may all be present for any single filing.
Visit each URL for each dataset on each Documents Button Page and download all the datasets listed there into the directories which were built in an earlier step.
Generate the Hadoop copyFromLocal commands to copy all the datasets from the step immediately before this one into the HDFS directories which were built in an earlier step.
There are four types of datasets created when this Protocol script (with LAB-1-A and LAB-1-B combined) is executed:

_Edgar.request file: Contains metadata about the script, the URL sent to Edgar and the directories and file names which will be used for the Request sent to and the Reply received from Edgar.
_Edgar.reply file: Contains the URL's for all the 'Documents' buttons present on the web page you would see if you just type the Request URL into your browser. With the LAB-1-B Request URL we built in LAB-1-A, we filtered for '10-K' filings which include forms 10-K and 10-K/A.
.metadata files: Contains metadata values used by Chameleon Metadata and will be covered later in the DSB-3 tutorial series.
.hadoop files: Contains the HDFS mkdir commands to make the HDFS directories into which the Edgar datasets will be loaded and the copyFromLocal commands which will copy all the downloaded datasets into HDFS.
